{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMwtj5rJs2TvGLD4hZeAvQ+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2B-RT1hZUujy"},"outputs":[],"source":["def convert2matrix(data_arr, look_back):\n","   X, Y =[], []\n","   for i in range(len(data_arr)-look_back):\n","     d=i+look_back\n","     X.append(data_arr[i:d,])\n","     Y.append(data_arr[d,])\n","   return np.array(X), np.array(Y)\n","\n","def model_lstm_uni(look_back):\n","    tf.keras.backend.clear_session()\n","    model=Sequential()\n","    model.add(LSTM(100, input_shape=(1, look_back), activation='relu'))\n","    model.add(Dense(1))\n","    model.compile(loss='mean_squared_error',  optimizer='adam',metrics = ['mse', 'mae'])\n","    return model\n","\n","from sklearn.model_selection import TimeSeriesSplit\n","def forecast_error(data, first_rem=0.5):\n","    train, test = train_test_split(data, train_size=first_rem, shuffle=False)\n","    look_back = 5\n","    trainX, trainY = convert2matrix(np.array(train), look_back)\n","    testX, testY = convert2matrix(np.array(test), look_back)\n","    # reshape input to be [samples, time steps, features]\n","    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n","    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n","    model=model_lstm_uni(look_back)\n","    history = model.fit(trainX,\n","                      trainY,\n","                      epochs=100,\n","                      batch_size=30,\n","                      validation_data=(testX, testY),\n","                      callbacks=[EarlyStopping(monitor='val_loss', patience=10)],\n","                      verbose=0,\n","                      shuffle=False)\n","\n","    train_predict = model.predict(trainX)\n","    test_predict = model.predict(testX)\n","    error = (np.sqrt(mean_squared_error(testY, test_predict)))\n","\n","    return error,trainX, testX,testY, test_predict\n","\n","def multi_data_prep(s1, s2):\n","    # define input sequence\n","    in_seq1 = array(s1[:-1])\n","    in_seq2 = array(s2[:-1])\n","    out_seq = array(s1[1:])\n","    # convert to [rows, columns] structure\n","    in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n","    in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n","    out_seq = out_seq.reshape((len(out_seq), 1))\n","    # horizontally stack columns\n","    dataset = hstack((in_seq1, in_seq2, out_seq))\n","\n","    return dataset\n","\n","def multi_data_prep_three(s1, s2,s3):\n","    # define input sequence\n","    in_seq1 = array(s1[:-1])\n","    in_seq2 = array(s2[:-1])\n","    in_seq3 = array(s3[:-1])\n","    out_seq = array(s1[1:])\n","    # convert to [rows, columns] structure\n","    in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n","    in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n","    out_seq = out_seq.reshape((len(out_seq), 1))\n","    in_seq3 = in_seq3.reshape((len(in_seq3), 1))\n","    # horizontally stack columns\n","    dataset = hstack((in_seq1, in_seq2, in_seq3,out_seq))\n","\n","    return dataset\n","\n","# split a multivariate sequence into samples\n","def split_sequences(sequences, n_steps):\n","\tX, y = list(), list()\n","\tfor i in range(len(sequences)):\n","\t\t# find the end of this pattern\n","\t\tend_ix = i + n_steps\n","\t\t# check if we are beyond the dataset\n","\t\tif end_ix > len(sequences):\n","\t\t\tbreak\n","\t\t# gather input and output parts of the pattern\n","\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n","\t\tX.append(seq_x)\n","\t\ty.append(seq_y)\n","\treturn array(X), array(y)\n","\n","#Multivariate model\n","#Multivariate data preparation\n","\n","def forecast_error_multi(data, n_features, first_rem=0.5):\n","    train, test = train_test_split(data, train_size=first_rem, shuffle=False)\n","  # choose a number of time steps\n","    n_steps = 5\n","  # convert into input/output\n","    trainX, trainY = split_sequences(train, n_steps)\n","    testX, testY = split_sequences(test, n_steps)\n","\n","    # the dataset knows the number of features, e.g. 2\n","    #n_features = 2\n","    # define model\n","    tf.keras.backend.clear_session()\n","    model = Sequential()\n","    model.add(LSTM(100, activation='relu', input_shape=(n_steps, n_features)))\n","    model.add(Dense(1))\n","    model.compile(optimizer='adam', loss='mse')\n","    # fit model\n","    model.fit(trainX, trainY, epochs=100, verbose=0)\n","\n","    train_predict = model.predict(trainX)\n","    test_predict = model.predict(testX)\n","    error = (np.sqrt(mean_squared_error(testY, test_predict)))\n","\n","    print(error)\n","    return error,trainX, testX, testY, test_predict\n","\n","def calculations_cgc(inputs):\n","    df,input_bands = inputs\n","    #print(list_of_bands)\n","    list_of_bands = pd.DataFrame(input_bands).reset_index()\n","\n","    #for index, list_of_bands in enumerate(input_bands):\n","    for i in range(0, len(list_of_bands)):\n","        x_band = list_of_bands['X'][i]\n","        y_band = list_of_bands['Y'][i]\n","        z_band = list_of_bands['Z'][i]\n","\n","        print(x_band, y_band, z_band)\n","\n","\n","        two_sig_data = multi_data_prep(df[x_band], df[y_band])\n","        three_sig_data = multi_data_prep_three(df[x_band], df[y_band], df[z_band])\n","\n","        error1, x_train1, x_test1, y_test1, y_hat1 = forecast_error_multi(two_sig_data, 2)\n","        error2, x_train2, x_test2, y_test2, y_hat2 = forecast_error_multi(three_sig_data, 3)\n","\n","        cgc = (error1 - error2)\n","        percentage = (cgc/error1)*100\n","        list_of_bands['XY_X_Error'][i] = error1\n","        list_of_bands['XYZ_X_Error'][i] = error2\n","        list_of_bands['CGC_Value'][i] = cgc\n","        list_of_bands['Percentage Reduction'][i] = percentage\n","\n","    return list_of_bands\n","\n","\n","\n","def calculations_gc(inputs):\n","    df,input_bands = inputs\n","    #print(list_of_bands)\n","    list_of_bands = pd.DataFrame(input_bands).reset_index()\n","\n","    #for index, list_of_bands in enumerate(input_bands):\n","    for i in range(0, len(list_of_bands)):\n","        x_band = list_of_bands['X'][i]\n","        y_band = list_of_bands['Y'][i]\n","\n","        print(x_band, y_band)\n","\n","\n","        two_sig_data = multi_data_prep(df[x_band], df[y_band])\n","\n","        error1, x_train1, x_test1, y_test1, y_hat1 = forecast_error(df[x_band])\n","        error2, x_train2, x_test2, y_test2, y_hat2 = forecast_error_multi(two_sig_data, 2)\n","\n","        gc = (error1 - error2)\n","        percentage = (gc/error1)*100\n","        list_of_bands['X_X_Error'][i] = error1\n","        list_of_bands['XY_X_Error'][i] = error2\n","        list_of_bands['GC_Value'][i] = gc\n","        list_of_bands['Percentage Reduction'][i] = percentage\n","\n","\n","    return list_of_bands"]}]}